---
title: Starter Project for Sentiment Analysis
author: Tristan Tran
date: "12/14/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
bibliography: BibLaTex.bib
csl: journal-of-the-royal-society-interface.csl
nocite: '@*'
---


\section*{Introduction}
Sentiment analysis and text mining allows Data Scientists to classify text, novels, and other written forms of communication as data. Its practical applications can allow machine learning models and artificial intelligence understand how humans talk and how they feel. Imagine a world where mental health could be diagnosed from a few text messages, consumers could better find the products they need, and computers could speak a little more like humans. Although this project is merely an introductory level endeavor, it hopes to build a foundation to accomplish such feats in the future. 

\section*{Purpose}
The purpose of this project was to practice the basic techniques of data manipulation and working with text data. Text data often comes in forms that are difficult to work with and analyze. Text mining techniques and packages such as tidy text make this process a lot smoother.

The program is run by calling the following code.
```{r setup, echo=TRUE,message=FALSE,warning=FALSE,results=FALSE}
source("src/HorrorAnalysis.R")
```
The code reads through a csv "data/booklist.csv" and returns a word cloud for each book listed.

\section*{Important Packages}
For this project several packages were used to make the data readable and to generate the graphics


\subsection{dplyr}
The dplyr package is used to manipulate and modify data frames. The included mutate() function was used to produce the line number and chapter columns in the tidy versions of the book.

\subsection{gutenbergr}
The gutenbergr package gives the user access to the thousands of books that have been uploaded to project Gutenberg that aims to provide the world with free and easy to access classic literature so that our collective human culture and knowledge does not get lost. This package was used to import several classical horror novels from the included csv file. Below is an example of how one would import a single file. By calling the head function, it is apparent that the formatting and whitespace make it difficult to perform any sort of analysis.

```{r gutenberg}
drjekyll<-gutenberg_download(42,meta_fields ="title")
head(drjekyll,12)
```



\subsection{Tidy Things}
The function located in the sentiment_analysis.R file

```{r create}
create_tidy_books
```

The tidytext, stringr, and tidyr packages are all designed to make the data more readable. The tidytext website describes a tidy data format as one observation per row with one attribute per column. The function create_tidy_books utilizes the functions from these packages separate the words from their line numbers without losing any information, other than capitalization.

```{r maketidy}
tidy_jekyll <- create_tidy_books(drjekyll)

head(tidy_jekyll,12)
```
This package also comes with the get_sentiments function that queries text lexicons useful for the analysis.


\section{Graphics}
This project created two main types of graphics. Below
\subsection{Word Clouds}

```{r wordcloud, echo=FALSE}
create_wordcloud
```

The first type of data visualization involves getting a a word for each novel and creating a word cloud of the 100 most common words. The names of the protagonists and supporting cast are over-represented. To address this, a file "ignore.csv" was created. The word cloud will ignore anything in that file
```{r stop_words}
all_stop_words <- create_stop_words("data/ignore.csv")
tidy_jekyll_no_stop <- tidy_jekyll%>%
  anti_join(all_stop_words,by="word")

tibble(
  total_words = nrow(tidy_jekyll),
  after_cleanup= nrow(tidy_jekyll_no_stop)
)
```
By cleaning up the data and removing stop words, we can reduce the dimensionality of the data and make analysis easier. We can also see the change in the word cloud.

```{r creatcloud, echo=FALSE,message=FALSE,warning=FALSE}
tidy_jekyll%>%
    filter(gutenberg_id == 42) %>%
    anti_join(stop_words) %>%
    count(word)%>%
    with(wordcloud(word, n, max.words = 100,min.freq = 10,scale=c(4,.5)))

```

```{r creatcloud2, echo=FALSE,message=FALSE,warning=FALSE}
tidy_jekyll_no_stop%>%
    filter(gutenberg_id == 42) %>%
    anti_join(stop_words) %>%
    count(word)%>%
    with(wordcloud(word, n, max.words = 100,min.freq = 10,scale=c(4,.5)))

```

\subsection{Plot Progression and Trajectory}
\subsubsection{Figure: Trajectory Graph of Raw Text}
```{r progression,echo=FALSE,message=FALSE,warning=FALSE}
horror_books <- gutenberg_download(horror_book_id, meta_fields = "title")
all_stop_words <- create_stop_words("data/ignore.csv")
tidy_horror <- create_tidy_books(horror_books)

horror_sentiment <- get_net_sentiment(tidy_horror,80)


#graph the net positive affinity for each novel.f
ggplot(horror_sentiment, aes(index, sentiment, fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~title, ncol = 2, scales = "free_x")
```



The bing lexicon provides a negative or positive designation for English words. These scores were obtained in 2004. The sentiments are relevant for that time period and can be anachronistic compared to some classic literature. For instance, English speakers in 2004 might view the word miss as a negative word meaning "fail to hit or notice"; however, in older settings and literature this can mean "young woman of nobility". To address these anachronisms, they will be similarly added to the ignore.csv file and processed in the same way. Below is the result of plot trajectory when the stop words library is applied

\subsubsection{Figure: Trajectory Graph of Cleaned Text}
```{r progressionclean,echo=FALSE,message=FALSE,warning=FALSE}

tidy_horror <- tidy_horror %>%
  anti_join(all_stop_words,by="word")

horror_sentiment <- get_net_sentiment(tidy_horror,80)


#graph the net positive affinity for each novel.f
ggplot(horror_sentiment, aes(index, sentiment, fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~title, ncol = 2, scales = "free_x")
```

The result is closer to what one would expect from a horror novel. Further testing and processing is necessary to determine if this is appropriate for analysis or if it causes a worse interpretation of storylines.


\section{Difficulties and Opportunities}
Although the program is much better than its first iteration, there is still room for improvement. The lexicon with anachronistic words removed is still not as good as mining lexicons from a group of scholars proficient in the English language at the time each book was written. Unfortunately, machine learning is not good at handling out of sample data. The code is most likely better applied to modern literature or social media data. Another challenge that would improve the reading is the inclusion of bigrams. Bigrams allow one to analyze a series of words and measure their net meaning. While "happy" connotes joy, "not happy" connotes sadness. When measuring sentiment in a word by word algorithm, the net measurement might be zero; however, if they were analyzed as a bigram, the program could interpret the net effect as a negative emotion.


\section{References}

